{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import copy\n",
    "import glob\n",
    "import math\n",
    "import joblib\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from transformers import BertTokenizer, VisualBertForQuestionAnswering, VisualBertConfig\n",
    "import scipy\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.interpolate import CubicSpline, interp1d\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, roc_curve, classification_report, top_k_accuracy_score, coverage_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def scherrer_fwhm(crystal_size, theta, wavelength=1.5406, shape_factor=0.9):\n",
    "    theta_rad = np.deg2rad(theta/2)\n",
    "    fwhm = (shape_factor*wavelength)/(crystal_size*np.cos(theta_rad))\n",
    "    return fwhm\n",
    "\n",
    "def load_plt_setting():\n",
    "    plt.style.use('seaborn-white')\n",
    "    mpl.rcParams['font.sans-serif'] = \"Arial\"\n",
    "    mpl.rcParams['font.family'] = \"sans-serif\"\n",
    "    mpl.rcParams['axes.linewidth'] = 2\n",
    "    font = {'size': 32}\n",
    "    mpl.rc('font', **font)\n",
    "    mpl.rcParams['xtick.major.pad']='8'\n",
    "    mpl.rcParams['ytick.major.pad']='8'\n",
    "    plt.rcParams[\"font.weight\"] = \"normal\"\n",
    "    plt.rcParams[\"axes.labelweight\"] = \"normal\"\n",
    "    plt.rcParams['svg.fonttype'] = 'none'\n",
    "    mpl.rcParams['axes.linewidth'] = 2\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "_ = tokenizer.add_tokens('pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    '''\n",
    "    [Input]\n",
    "    dataset_path:      Path to dataset generated from preprocess.py\n",
    "                       dataset: 'formula', 'element_list', 'space_group', 'xrd_list'\n",
    "                                'xrd_list': list of (X,Y), len(xrd_list) depends on the strain setting\n",
    "    \n",
    "    batch_size:        Batch size n.\n",
    "    max_n_mix:         Maximum number of componds in XRD mix\n",
    "    \n",
    "    [Output]\n",
    "    Xs:                Array of intensity in (n,Y,1), n = batch_size\n",
    "    Ys:                Array of classification labels in (n, len(dataset))\n",
    "    element_list:      List of elements for each sample, n = len(element_list) \n",
    "    formula_list:      Human labels, n = len(formula_list)\n",
    "    '''\n",
    "    def __init__(self, dataset_path):\n",
    "        \n",
    "        try:\n",
    "            with open(dataset_path, 'rb') as handle:\n",
    "                self.dataset = joblib.load(handle)\n",
    "                print('Loading dataset successful.')\n",
    "        except:\n",
    "            print(\"Missing dataset.\")\n",
    "        \n",
    "        self.sample_list, self.sample_formula_list, self.combination_list = [], [], []\n",
    "        self.multiphase = {}\n",
    "        self.len = len(self.dataset)\n",
    "        \n",
    "        for sample, self.data in self.dataset.items():\n",
    "            self.sample_list.append(sample)\n",
    "            formula = self.data['formula']\n",
    "            if '-' in formula:\n",
    "                formula = formula.split('-')\n",
    "                formula = formula[-1] + '-' + formula[0]\n",
    "            self.sample_formula_list.append(re.findall(r'\\D+', formula) + re.findall(r'\\d+', formula))\n",
    "            \n",
    "        self.sample_list = [x for _, x in sorted(zip(self.sample_formula_list, self.sample_list))]\n",
    "        for i,sample in enumerate(self.sample_list):\n",
    "            self.elements = self.dataset[sample]['element_list']\n",
    "            self.multiphase[''.join(set(self.elements))] = {'elements':self.elements,'samples':[]}\n",
    "            print(i,self.dataset[sample]['formula'],self.elements)\n",
    "            \n",
    "        for sample in self.sample_list:\n",
    "            self.elements = self.dataset[sample]['element_list']\n",
    "            for combination in self.multiphase.keys():\n",
    "                if set(self.elements).issubset(set(self.multiphase[combination]['elements'])):\n",
    "                    self.multiphase[combination]['samples'].append(sample)    \n",
    "        \n",
    "        self.multiphase = {k: v for k, v in self.multiphase.items() if len(v.get('samples', [])) >= 2}\n",
    "        print(self.multiphase)\n",
    "        \n",
    "    def load_data(self, batch_size=10, twotheta=np.arange(5.00, 60.01, 0.01), \n",
    "                  n_mix=[1,2,3], resonable_mixing=False, min_mixing_ratio=0.05, \n",
    "                  high_orientation_probability=0.2, crystal_size_range=(5, 20), intensity_variation_range=(0.2, 1), \n",
    "                  noise_sigma_list=np.logspace(-4,-2,num=101)):\n",
    "        \n",
    "        self.n_mix_list = np.random.choice(n_mix, batch_size, replace=True)\n",
    "        \n",
    "        self.Xs = np.zeros((batch_size,len(twotheta),1))\n",
    "        self.Ys = np.zeros((batch_size,self.len))\n",
    "        self.element_list, self.formula_list = [], []\n",
    "        \n",
    "        i = 0\n",
    "        while i < batch_size:\n",
    "            if resonable_mixing and n_mix!=[1]:\n",
    "                self.sample_idxs = []\n",
    "                samples = self.multiphase[np.random.choice(list(self.multiphase.keys()))]['samples']\n",
    "                self.n_mix_list[i] = min(self.n_mix_list[i],len(samples))\n",
    "                samples = np.random.choice(samples, self.n_mix_list[i], replace=False)\n",
    "                for sample in samples:\n",
    "                    self.sample_idxs.append(self.sample_list.index(sample))\n",
    "            else:\n",
    "                self.sample_idxs = np.random.choice(self.len, size=self.n_mix_list[i], replace=False)\n",
    "            self.formulas, self.elements = [], []\n",
    "            \n",
    "            self.mixing_ratio = np.random.uniform(min_mixing_ratio,1,len(self.sample_idxs))\n",
    "            self.mixing_ratio = self.mixing_ratio/np.sum(self.mixing_ratio)\n",
    "            \n",
    "            for j, sample_idx in enumerate(self.sample_idxs):\n",
    "                self.data = self.dataset[self.sample_list[sample_idx]]\n",
    "                self.formulas.append(self.data['formula'])\n",
    "                for element in self.data['element_list']:\n",
    "                    if element not in self.elements:\n",
    "                        self.elements.append(element)\n",
    "                self.Ys[i, sample_idx] += 1\n",
    "                \n",
    "                self.twotheta_short, self.X_short = self.data['xrd_list'][np.random.randint(0,len(self.data['xrd_list']))]\n",
    "                if np.random.binomial(1, high_orientation_probability) == 1:\n",
    "                    high_orientation_peak_index = np.random.choice(np.argpartition(dataloader.X_short,-3)[-3:],1)[0]\n",
    "                    self.X_short = np.array([self.X_short[high_orientation_peak_index]])\n",
    "                    self.twotheta_short = np.array([self.twotheta_short[high_orientation_peak_index]])\n",
    "                \n",
    "                self.X = np.zeros(twotheta.shape)\n",
    "                self.X[np.searchsorted(twotheta,self.twotheta_short)] = self.X_short\n",
    "                \n",
    "                # Intensity variation\n",
    "                self.X = self.X * np.random.uniform(*intensity_variation_range, self.X.shape[0])\n",
    "                # Crystal size broadening\n",
    "                fwhm = scherrer_fwhm(np.random.uniform(*crystal_size_range), twotheta)\n",
    "                sigma = np.mean(fwhm)/(2*np.sqrt(2*np.log(2)))\n",
    "                self.X = gaussian_filter(self.X, sigma=sigma*100)\n",
    "                \n",
    "                self.Xs[i,:,0] += self.X/np.max(self.X)*self.mixing_ratio[j]\n",
    "            \n",
    "            if np.max(self.Xs[i,:,0]) == 0:\n",
    "                self.Ys[i, :] = np.zeros(self.Ys[i, :].shape)\n",
    "                pass\n",
    "            else:\n",
    "                self.Xs[i,:,0] = self.Xs[i,:,0]/np.max(self.Xs[i,:,0]) + np.random.normal(0, np.random.choice(noise_sigma_list), len(twotheta))\n",
    "                self.Xs[i,:,0] = (self.Xs[i,:,0]-np.min(self.Xs[i,:,0]))/(np.max(self.Xs[i,:,0])-np.min(self.Xs[i,:,0]))\n",
    "                self.formula_list.append(self.formulas)\n",
    "                self.element_list.append(list(set(np.array(self.elements).flatten())))\n",
    "                i += 1\n",
    "            \n",
    "        return self.Xs, self.Ys, self.element_list, self.formula_list, self.n_mix_list\n",
    "    \n",
    "    def load_ref(self, sample_idx=0, twotheta=np.arange(5.00, 60.01, 0.01),):\n",
    "        \n",
    "        self.data = self.dataset[self.sample_list[sample_idx]]\n",
    "        self.twotheta_short, self.X_short = self.data['xrd_list'][int(len(dataloader.data['xrd_list'])/2+0.5)]\n",
    "        self.X = np.zeros(twotheta.shape)\n",
    "        self.X[np.searchsorted(twotheta,self.twotheta_short)] = self.X_short\n",
    "        self.X = self.X/np.max(self.X)\n",
    "        return self.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder = os.getcwd()\n",
    "cif_folder = os.path.join(folder, 'cif')\n",
    "dataloader = DataLoader(os.path.join(cif_folder, 'dataset.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sample_size in [1e2,3e2,1e3,3e3,1e4,3e4,1e5,3e5,1e6,3e6,1e7]:\n",
    "    batch_size = int(sample_size)\n",
    "    if sample_size > 1000:\n",
    "        num_of_epochs = int(batch_size/1000)\n",
    "        batch_size = 1000\n",
    "    else:\n",
    "        num_of_epochs = 1\n",
    "        \n",
    "    clf = SGDClassifier(learning_rate='adaptive', eta0=0.01)\n",
    "    \n",
    "    time_start = time.time()\n",
    "    for i in range(num_of_epochs):\n",
    "        Xs, Ys, element_list, formula_list, _ = dataloader.load_data(batch_size=batch_size, n_mix=[1],\n",
    "                                                              high_orientation_probability=0.2, crystal_size_range=(5, 20), \n",
    "                                                              intensity_variation_range=(0.01, 1))\n",
    "        Ys_int = [np.where(x==1)[0][0] for x in Ys]\n",
    "\n",
    "        clf.partial_fit(Xs[:,:-1,0], Ys_int, classes=np.linspace(0,65,66).astype(int))\n",
    "\n",
    "        if (i+1)%10 == 0:\n",
    "            print('{}\\t{}'.format(i+1, time.time()-time_start))\n",
    "\n",
    "    with open(os.path.join(folder, 'baselines', 'SVM', 'SVM_{}.pkl'.format(int(sample_size))),'wb') as f:\n",
    "        pickle.dump(clf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(cif_folder, 'test_dataset_1phase_orientation.npy'), 'rb') as handle:\n",
    "    test_dataset = joblib.load(handle)\n",
    "    \n",
    "Xs, Ys, element_list, formula_list = test_dataset['Xs'], test_dataset['Ys'], test_dataset['element_list'], test_dataset['formula_list']\n",
    "Ys_int = [np.where(x==1)[0][0] for x in Ys]\n",
    "\n",
    "\n",
    "for sample_size in [1e2,3e2,1e3,3e3,1e4,3e4,1e5,3e5,1e6,3e6,1e7]:\n",
    "\n",
    "    clf = pickle.load(open(os.path.join(folder, 'baselines', 'SVM', 'SVM_{}.pkl'.format(int(sample_size))), 'rb'))\n",
    "\n",
    "    Ys_pred_int = clf.predict(Xs[:,:-1,0])\n",
    "\n",
    "    load_plt_setting()\n",
    "\n",
    "    report = classification_report(Ys_int, Ys_pred_int, target_names=dataloader.sample_list, output_dict=True)\n",
    "    matrix = confusion_matrix(Ys_int, Ys_pred_int)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    mat = ax.matshow(matrix, cmap='hot')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10, steps=[1, 2, 5, 10]))\n",
    "    ax.xaxis.set_minor_locator(mpl.ticker.AutoMinorLocator(2))\n",
    "    ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10, steps=[1, 2, 5, 10]))\n",
    "    ax.yaxis.set_minor_locator(mpl.ticker.AutoMinorLocator(2))\n",
    "    ax.tick_params(axis='both',direction='out',length=8,width=2,pad=10,color='black',labelsize=28)\n",
    "    ax.tick_params(axis='both',which='minor',direction='out',length=4,width=2,pad=10,color='black',labelsize=28)\n",
    "    # ax.axes.set_xlim([450,950])\n",
    "    # ax.axes.set_ylim([0,5])\n",
    "    # # ax.set_xscale('log')\n",
    "\n",
    "    cbar = plt.colorbar(mat)\n",
    "    cbar.ax.tick_params(axis='y', direction='out',length=8,width=3,pad=5,labelsize=28)\n",
    "    cbar.ax.set_ylabel('Number of samples',labelpad=40, rotation=-90)\n",
    "    cbar.ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=4, integer=True, steps=[1, 2, 5, 10]))\n",
    "    cbar.ax.yaxis.set_minor_locator(mpl.ticker.AutoMinorLocator(2))\n",
    "    cbar.ax.tick_params(axis='both',which='minor',direction='out',length=4,width=3,pad=5)\n",
    "\n",
    "    ax.set_xlabel(r'Predicted class', labelpad=20, fontsize=34)\n",
    "    ax.set_ylabel(r'True class', labelpad=20, fontsize=34)\n",
    "\n",
    "    plt.text(0.95, 0.91, 'Accuracy: {:.1f}%'.format(report['accuracy']*100), fontsize=32, \n",
    "             transform=ax.transAxes, color='#FFFFFF', horizontalalignment='right')\n",
    "    if sample_size%1000000 == 0:\n",
    "        data_size = str(int(sample_size//1000000))+'M'\n",
    "    elif sample_size%1000 == 0:\n",
    "        data_size = str(int(sample_size//1000))+'k'\n",
    "    else:\n",
    "        data_size = int(sample_size)\n",
    "    plt.text(0.05, 0.06, 'Data size: {}'.format(data_size), fontsize=32, transform=ax.transAxes, color='#FFFFFF', horizontalalignment='left')\n",
    "\n",
    "    plt.savefig(os.path.join(folder, 'baselines', 'SVM', 'SVM_{}.png'.format(int(sample_size))), format='png', dpi=300, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, n_class):\n",
    "        super(Model, self).__init__()\n",
    "        self.con1 = nn.LazyConv1d(64, 50, stride=2)\n",
    "        self.poo1 = nn.MaxPool1d(3,stride=2)\n",
    "        self.con2 = nn.LazyConv1d(64, 25, stride=3)\n",
    "        self.poo2 = nn.MaxPool1d(2,stride=3)\n",
    "        self.fc1 = nn.LazyLinear(2000)\n",
    "        self.fc2 = nn.LazyLinear(500)\n",
    "        self.fc3 = nn.LazyLinear(n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.con1(x)\n",
    "        x = self.poo1(x)\n",
    "        x = self.con2(x)\n",
    "        x = self.poo2(x)\n",
    "        x = self.fc1(torch.flatten(x,start_dim=1))\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses,train_y_true,train_y_pred = [],[],[]\n",
    "\n",
    "for sample_size in [1e2,3e2,1e3,3e3,1e4,3e4,1e5,3e5,1e6,3e6,1e7]:\n",
    "# for sample_size in [1e4]:\n",
    "\n",
    "    Xs, Ys, element_list, formula_list, _ = dataloader.load_data(batch_size=1, n_mix=[1], high_orientation_probability=0.2, \n",
    "                                                                 crystal_size_range=(5, 20),intensity_variation_range=(0.01, 1))\n",
    "    Ys_int = [np.where(x==1)[0][0] for x in Ys]\n",
    "\n",
    "    model = Model(Xs.shape[1]-1,Ys.shape[1]).to('cuda')\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    batch_size = 20\n",
    "    num_of_epochs = int(sample_size/batch_size)\n",
    "\n",
    "    time_start = time.time()\n",
    "    for i in range(num_of_epochs):\n",
    "        Xs, Ys, element_list, formula_list, _ = dataloader.load_data(batch_size=batch_size, n_mix=[1],high_orientation_probability=0.2, \n",
    "                                                                     crystal_size_range=(5, 20),intensity_variation_range=(0.01, 1))\n",
    "\n",
    "        Y_predict = model(torch.swapaxes(torch.tensor(Xs[:,:-1,:], dtype=torch.float32),1,2).to('cuda'))\n",
    "        loss = nn.CrossEntropyLoss()(Y_predict, torch.tensor(Ys,dtype=torch.float32).to('cuda'))\n",
    "\n",
    "        labels = torch.from_numpy(np.array(Ys)).to(dtype=torch.float32)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_true = Ys\n",
    "        train_y_true.append(Ys)\n",
    "        y_pred = np.array(Y_predict.detach().cpu())\n",
    "        train_y_pred.append(y_pred)\n",
    "        train_losses.append(np.array(loss.detach().cpu()))\n",
    "        \n",
    "        if (i+1)%10 == 0:\n",
    "            print('{}\\t{:.5f}\\t{}'.format(i+1, np.array(loss.detach().cpu()), y_true.argmax(-1)[:15]-y_pred.argmax(-1)[:15]))\n",
    "\n",
    "    torch.save(model, os.path.join(folder, 'baselines', 'CNN', '{}.pt'.format(int(sample_size))))\n",
    "    np.savetxt(os.path.join(folder, 'baselines', 'CNN', 'train_loss_{}.csv'.format(int(sample_size))), train_losses, delimiter=',')\n",
    "    \n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(cif_folder, 'test_dataset_1phase_orientation.npy'), 'rb') as handle:\n",
    "    test_dataset = joblib.load(handle)\n",
    "    \n",
    "Xs, Ys, element_list, formula_list = test_dataset['Xs'], test_dataset['Ys'], test_dataset['element_list'], test_dataset['formula_list']\n",
    "Ys_int = [np.where(x==1)[0][0] for x in Ys]\n",
    "\n",
    "\n",
    "for sample_size in [1e2,3e2,1e3,3e3,1e4,3e4,1e5,3e5,1e6,3e6,1e7]:\n",
    "\n",
    "    model = torch.load(os.path.join(folder, 'baselines', 'CNN', '{}.pt'.format(int(sample_size))))\n",
    "\n",
    "    Ys_pred = np.array(model(torch.swapaxes(torch.tensor(Xs[:,:-1,:], dtype=torch.float32),1,2).to('cuda')).detach().cpu())\n",
    "    Ys_pred_int = np.argmax(Ys_pred, axis=1)\n",
    "    \n",
    "    load_plt_setting()\n",
    "\n",
    "    report = classification_report(Ys_int, Ys_pred_int, target_names=dataloader.sample_list, output_dict=True)\n",
    "    matrix = confusion_matrix(Ys_int, Ys_pred_int)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    mat = ax.matshow(matrix, cmap='hot')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10, steps=[1, 2, 5, 10]))\n",
    "    ax.xaxis.set_minor_locator(mpl.ticker.AutoMinorLocator(2))\n",
    "    ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10, steps=[1, 2, 5, 10]))\n",
    "    ax.yaxis.set_minor_locator(mpl.ticker.AutoMinorLocator(2))\n",
    "    ax.tick_params(axis='both',direction='out',length=8,width=2,pad=10,color='black',labelsize=28)\n",
    "    ax.tick_params(axis='both',which='minor',direction='out',length=4,width=2,pad=10,color='black',labelsize=28)\n",
    "    # ax.axes.set_xlim([450,950])\n",
    "    # ax.axes.set_ylim([0,5])\n",
    "    # # ax.set_xscale('log')\n",
    "\n",
    "    cbar = plt.colorbar(mat)\n",
    "    cbar.ax.tick_params(axis='y', direction='out',length=8,width=3,pad=5,labelsize=28)\n",
    "    cbar.ax.set_ylabel('Number of samples',labelpad=40, rotation=-90)\n",
    "    cbar.ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=4, integer=True, steps=[1, 2, 5, 10]))\n",
    "    cbar.ax.yaxis.set_minor_locator(mpl.ticker.AutoMinorLocator(2))\n",
    "    cbar.ax.tick_params(axis='both',which='minor',direction='out',length=4,width=3,pad=5)\n",
    "\n",
    "    ax.set_xlabel(r'Predicted class', labelpad=20, fontsize=34)\n",
    "    ax.set_ylabel(r'True class', labelpad=20, fontsize=34)\n",
    "\n",
    "    plt.text(0.95, 0.91, 'Accuracy: {:.1f}%'.format(report['accuracy']*100), fontsize=32, \n",
    "             transform=ax.transAxes, color='#FFFFFF', horizontalalignment='right')\n",
    "    if sample_size%1000000 == 0:\n",
    "        data_size = str(int(sample_size//1000000))+'M'\n",
    "    elif sample_size%1000 == 0:\n",
    "        data_size = str(int(sample_size//1000))+'k'\n",
    "    else:\n",
    "        data_size = int(sample_size)\n",
    "    plt.text(0.05, 0.06, 'Data size: {}'.format(data_size), fontsize=32, transform=ax.transAxes, color='#FFFFFF', horizontalalignment='left')\n",
    "\n",
    "    plt.savefig(os.path.join(folder, 'baselines', 'CNN', 'CNN_{}.png'.format(int(sample_size))), format='png', dpi=300, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nd2]",
   "language": "python",
   "name": "conda-env-nd2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
