{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Tools\\Anaconda\\envs\\nd2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import copy\n",
    "import glob\n",
    "import math\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from transformers import BertTokenizer, VisualBertForQuestionAnswering, VisualBertConfig\n",
    "import scipy\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, roc_curve, classification_report, top_k_accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def scherrer_fwhm(crystal_size, theta, wavelength=1.5406, shape_factor=0.9):\n",
    "    theta_rad = np.deg2rad(theta/2)\n",
    "    fwhm = (shape_factor*wavelength)/(crystal_size*np.cos(theta_rad))\n",
    "    return fwhm\n",
    "\n",
    "def load_plt_setting():\n",
    "    plt.style.use('seaborn-white')\n",
    "    mpl.rcParams['font.sans-serif'] = \"Arial\"\n",
    "    mpl.rcParams['font.family'] = \"sans-serif\"\n",
    "    mpl.rcParams['axes.linewidth'] = 2\n",
    "    font = {'size': 32}\n",
    "    mpl.rc('font', **font)\n",
    "    mpl.rcParams['xtick.major.pad']='8'\n",
    "    mpl.rcParams['ytick.major.pad']='8'\n",
    "    plt.rcParams[\"font.weight\"] = \"normal\"\n",
    "    plt.rcParams[\"axes.labelweight\"] = \"normal\"\n",
    "    plt.rcParams['svg.fonttype'] = 'none'\n",
    "    mpl.rcParams['axes.linewidth'] = 2\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "_ = tokenizer.add_tokens('pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    '''\n",
    "    [Input]\n",
    "    dataset_path:      Path to dataset generated from preprocess.py\n",
    "                       dataset: 'formula', 'element_list', 'space_group', 'xrd_list'\n",
    "                                'xrd_list': list of (X,Y), len(xrd_list) depends on the strain setting\n",
    "    \n",
    "    batch_size:        Batch size n.\n",
    "    max_n_mix:         Maximum number of componds in XRD mix\n",
    "    \n",
    "    [Output]\n",
    "    Xs:                Array of intensity in (n,Y,1), n = batch_size\n",
    "    Ys:                Array of classification labels in (n, len(dataset))\n",
    "    element_list:      List of elements for each sample, n = len(element_list) \n",
    "    formula_list:      Human labels, n = len(formula_list)\n",
    "    '''\n",
    "    def __init__(self, dataset_path):\n",
    "        \n",
    "        try:\n",
    "            with open(dataset_path, 'rb') as handle:\n",
    "                self.dataset = joblib.load(handle)\n",
    "                print('Loading dataset successful.')\n",
    "        except:\n",
    "            print(\"Missing dataset.\")\n",
    "        \n",
    "        self.sample_list, self.sample_formula_list, self.combination_list = [], [], []\n",
    "        self.multiphase = {}\n",
    "        self.len = len(self.dataset)\n",
    "        \n",
    "        for sample, self.data in self.dataset.items():\n",
    "            self.sample_list.append(sample)\n",
    "            formula = self.data['formula']\n",
    "            if '-' in formula:\n",
    "                formula = formula.split('-')\n",
    "                formula = formula[-1] + '-' + formula[0]\n",
    "            self.sample_formula_list.append(re.findall(r'\\D+', formula) + re.findall(r'\\d+', formula))\n",
    "            \n",
    "        self.sample_list = [x for _, x in sorted(zip(self.sample_formula_list, self.sample_list))]\n",
    "        for i,sample in enumerate(self.sample_list):\n",
    "            self.elements = self.dataset[sample]['element_list']\n",
    "            self.multiphase[''.join(set(self.elements))] = {'elements':self.elements,'samples':[]}\n",
    "            print(i,self.dataset[sample]['formula'],self.elements)\n",
    "            \n",
    "        for sample in self.sample_list:\n",
    "            self.elements = self.dataset[sample]['element_list']\n",
    "            for combination in self.multiphase.keys():\n",
    "                if set(self.elements).issubset(set(self.multiphase[combination]['elements'])):\n",
    "                    self.multiphase[combination]['samples'].append(sample)    \n",
    "        \n",
    "        self.multiphase = {k: v for k, v in self.multiphase.items() if len(v.get('samples', [])) >= 2}\n",
    "        print(self.multiphase)\n",
    "        \n",
    "    def load_data(self, batch_size=10, twotheta=np.arange(5.00, 60.01, 0.01), \n",
    "                  n_mix=[1,2,3], resonable_mixing=False, min_mixing_ratio=0.05, \n",
    "                  high_orientation_probability=0.2, crystal_size_range=(5, 20), intensity_variation_range=(0.2, 1), \n",
    "                  noise_sigma_list=np.logspace(-4,-2,num=101)):\n",
    "        \n",
    "        self.n_mix_list = np.random.choice(n_mix, batch_size, replace=True)\n",
    "        \n",
    "        self.Xs = np.zeros((batch_size,len(twotheta),1))\n",
    "        self.Ys = np.zeros((batch_size,self.len))\n",
    "        self.element_list, self.formula_list = [], []\n",
    "        \n",
    "        i = 0\n",
    "        while i < batch_size:\n",
    "            if resonable_mixing:\n",
    "                self.sample_idxs = []\n",
    "                samples = self.multiphase[np.random.choice(list(self.multiphase.keys()))]['samples']\n",
    "                self.n_mix_list[i] = min(self.n_mix_list[i],len(samples))\n",
    "                samples = np.random.choice(samples, self.n_mix_list[i], replace=False)\n",
    "                for sample in samples:\n",
    "                    self.sample_idxs.append(self.sample_list.index(sample))\n",
    "            else:\n",
    "                self.sample_idxs = np.random.choice(self.len, size=self.n_mix_list[i], replace=False)\n",
    "            self.formulas, self.elements = [], []\n",
    "            \n",
    "            self.mixing_ratio = np.random.uniform(min_mixing_ratio,1,len(self.sample_idxs))\n",
    "            self.mixing_ratio = self.mixing_ratio/np.sum(self.mixing_ratio)\n",
    "            \n",
    "            for j, sample_idx in enumerate(self.sample_idxs):\n",
    "                self.data = self.dataset[self.sample_list[sample_idx]]\n",
    "                self.formulas.append(self.data['formula'])\n",
    "                for element in self.data['element_list']:\n",
    "                    if element not in self.elements:\n",
    "                        self.elements.append(element)\n",
    "                self.Ys[i, sample_idx] += 1\n",
    "                \n",
    "                self.twotheta_short, self.X_short = self.data['xrd_list'][np.random.randint(0,len(self.data['xrd_list']))]\n",
    "                if np.random.binomial(1, high_orientation_probability) == 1:\n",
    "                    high_orientation_peak_index = np.random.choice(np.argpartition(dataloader.X_short,-3)[-3:],1)[0]\n",
    "                    self.X_short = np.array([self.X_short[high_orientation_peak_index]])\n",
    "                    self.twotheta_short = np.array([self.twotheta_short[high_orientation_peak_index]])\n",
    "                \n",
    "                self.X = np.zeros(twotheta.shape)\n",
    "                self.X[np.searchsorted(twotheta,self.twotheta_short)] = self.X_short\n",
    "                \n",
    "                # Intensity variation\n",
    "                self.X = self.X * np.random.uniform(*intensity_variation_range, self.X.shape[0])\n",
    "                # Crystal size broadening\n",
    "                fwhm = scherrer_fwhm(np.random.uniform(*crystal_size_range), twotheta)\n",
    "                sigma = np.mean(fwhm)/(2*np.sqrt(2*np.log(2)))\n",
    "                self.X = gaussian_filter(self.X, sigma=sigma*100)\n",
    "                \n",
    "                self.Xs[i,:,0] += self.X/np.max(self.X)*self.mixing_ratio[j]\n",
    "            \n",
    "            if np.max(self.Xs[i,:,0]) == 0:\n",
    "                self.Ys[i, :] = np.zeros(self.Ys[i, :].shape)\n",
    "                pass\n",
    "            else:\n",
    "                self.Xs[i,:,0] = self.Xs[i,:,0]/np.max(self.Xs[i,:,0]) + np.random.normal(0, np.random.choice(noise_sigma_list), len(twotheta))\n",
    "                self.Xs[i,:,0] = (self.Xs[i,:,0]-np.min(self.Xs[i,:,0]))/(np.max(self.Xs[i,:,0])-np.min(self.Xs[i,:,0]))\n",
    "                self.formula_list.append(self.formulas)\n",
    "                self.element_list.append(list(set(np.array(self.elements).flatten())))\n",
    "                i += 1\n",
    "            \n",
    "        return self.Xs, self.Ys, self.element_list, self.formula_list, self.n_mix_list\n",
    "    \n",
    "    def load_ref(self, sample_idx=0, twotheta=np.arange(5.00, 60.01, 0.01),):\n",
    "        \n",
    "        self.data = self.dataset[self.sample_list[sample_idx]]\n",
    "        self.twotheta_short, self.X_short = self.data['xrd_list'][int(len(dataloader.data['xrd_list'])/2+0.5)]\n",
    "        self.X = np.zeros(twotheta.shape)\n",
    "        self.X[np.searchsorted(twotheta,self.twotheta_short)] = self.X_short\n",
    "        self.X = self.X/np.max(self.X)\n",
    "        return self.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder = os.getcwd()\n",
    "cif_folder = os.path.join(folder, 'cif')\n",
    "\n",
    "dataloader = DataLoader(os.path.join(cif_folder, 'dataset.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xs, Ys, element_list, formula_list, n_mix_list = dataloader.load_data(batch_size=3, n_mix=[1], resonable_mixing=True,\n",
    "                                                          high_orientation_probability=0.3, noise_sigma_list=np.logspace(-4,-1,num=101))\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "for i in range(len(element_list)):\n",
    "    ax.plot(np.arange(5.00, 60.01, 0.01), Xs[i])\n",
    "#     print(element_list[i], formula_list[i], Ys[i])\n",
    "    \n",
    "ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=7, steps=[1, 2, 5, 10]))\n",
    "ax.xaxis.set_minor_locator(mpl.ticker.AutoMinorLocator(2))\n",
    "ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=1, steps=[1, 2, 5, 10]))\n",
    "ax.yaxis.set_minor_locator(mpl.ticker.AutoMinorLocator(2))\n",
    "ax.tick_params(axis='both',direction='in',length=8,width=2,pad=10,color='black',right='on',top='on',labelsize=28)\n",
    "ax.tick_params(axis='both',which='minor',direction='in',length=4,width=2,pad=10,color='black',right='on',top='on',labelsize=28)\n",
    "    \n",
    "ax.axes.set_xlim([5,60])\n",
    "ax.axes.set_ylim([-0.02,1.02])\n",
    "\n",
    "ax.set_xlabel(r'2$\\Theta$ (degree)', labelpad=20, fontsize=34)\n",
    "ax.set_ylabel(r'Normalized intensity', labelpad=0, fontsize=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Single-label train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embedding_dim = 20\n",
    "configuration = VisualBertConfig(vocab_size=len(tokenizer), visual_embedding_dim=visual_embedding_dim, hidden_size=int(Xs.shape[1]/visual_embedding_dim), \n",
    "                                 num_attention_heads=1, num_labels=Ys.shape[1])\n",
    "print(configuration)\n",
    "model = VisualBertForQuestionAnswering(configuration).to('cuda')\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_losses,train_y_true,train_y_pred = [],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_of_epochs = 100000\n",
    "batch_size = 20\n",
    "n_mix = [1]\n",
    "# prompt_type_list = [None,'eds','full']\n",
    "prompt_type_list = ['eds']\n",
    "organic_list = ['FA','MA']\n",
    "\n",
    "model_folder = r'202409078 single phase eds label'\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in range(num_of_epochs):\n",
    "    Xs, Ys, element_list, formula_list, _ = dataloader.load_data(batch_size=batch_size, n_mix=n_mix,\n",
    "                                                              high_orientation_probability=0.2, crystal_size_range=(5, 20), \n",
    "                                                              intensity_variation_range=(0.01, 1))\n",
    "    Xs = Xs[:,:-1,:].reshape((Xs.shape[0],int(Xs.shape[1]/visual_embedding_dim),visual_embedding_dim))\n",
    "    \n",
    "    X_prompts = []\n",
    "    for j in range(batch_size):\n",
    "        random.shuffle(element_list[j])\n",
    "        prompt_type = np.random.choice(prompt_type_list)\n",
    "        if prompt_type == None:\n",
    "            X_prompts.append('')\n",
    "        elif prompt_type == 'eds':\n",
    "            X_prompts.append(' '.join([element for element in element_list[j] if element not in ['FA', 'MA']]))\n",
    "        elif prompt_type == 'full':\n",
    "            X_prompts.append(' '.join(element_list[j]))\n",
    "\n",
    "    visual_embeds = torch.from_numpy(Xs).to(dtype=torch.float32)\n",
    "    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "            \n",
    "    inputs = tokenizer(X_prompts, return_tensors=\"pt\", padding=True)\n",
    "    inputs.update(\n",
    "        {\n",
    "            \"visual_embeds\": visual_embeds,\n",
    "            \"visual_token_type_ids\": visual_token_type_ids,\n",
    "            \"visual_attention_mask\": visual_attention_mask,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    labels = torch.from_numpy(np.array(Ys)).to(dtype=torch.float32)\n",
    "\n",
    "    outputs = model(**inputs.to('cuda'), labels=labels.to('cuda'))\n",
    "    logits = outputs.logits\n",
    "    if max_n_mix > 1:\n",
    "        weight = labels*9+1\n",
    "        loss = torch.nn.BCEWithLogitsLoss(weight=weight.to('cuda'))(logits, labels.to('cuda'))\n",
    "    else:\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_true = np.array(labels)\n",
    "    train_y_true.append(y_true)\n",
    "    y_pred = np.array(logits.detach().cpu())\n",
    "    train_y_pred.append(y_pred)\n",
    "    \n",
    "    print('{}\\t{:.5f}\\t{}'.format(i+1, np.array(loss.detach().cpu()), y_true.argmax(-1)[:15]-y_pred.argmax(-1)[:15]))\n",
    "#     break\n",
    "    train_losses.append(np.array(loss.detach().cpu()))\n",
    "    \n",
    "    if (i+1)%5000 == 0:\n",
    "        model.save_pretrained(os.path.join(folder, 'models', model_folder, '{}'.format(i+1)), from_pt=True) \n",
    "        np.savetxt(os.path.join(folder, 'models', model_folder, 'train_loss.csv'), train_losses, delimiter=',')\n",
    "        with open(os.path.join(folder, 'models', model_folder, 'train_result.npy'), 'wb') as handle:\n",
    "            joblib.dump({'y_true':train_y_true, 'y_pred':train_y_pred, 'loss':train_losses}, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Multi-label train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embedding_dim = 20\n",
    "\n",
    "model = VisualBertForQuestionAnswering.from_pretrained(os.path.join(folder,'pretrained_models','20240907_single_phase_diverse_prompt')).to('cuda')\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_losses,train_y_true,train_y_pred = [],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embedding_dim = 20\n",
    "n_mix = [3]\n",
    "configuration = VisualBertConfig(vocab_size=len(tokenizer), visual_embedding_dim=visual_embedding_dim, hidden_size=int(Xs.shape[1]/visual_embedding_dim), \n",
    "                                 num_attention_heads=1, num_labels=Ys.shape[1])\n",
    "print(configuration)\n",
    "model = VisualBertForQuestionAnswering(configuration).to('cuda')\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_losses,train_y_true,train_y_pred = [],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_of_epochs = 100000\n",
    "\n",
    "batch_size = 20\n",
    "max_n_mix = 3\n",
    "prompt_type_list = [None,'eds','full']\n",
    "# prompt_type_list = ['eds']\n",
    "organic_list = ['FA','MA']\n",
    "\n",
    "model_folder = r'20240909 multi phase reasonable mixing'\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in range(num_of_epochs):\n",
    "    Xs, Ys, element_list, formula_list, _ = dataloader.load_data(batch_size=batch_size, n_mix=n_mix, resonable_mixing=True,\n",
    "                                                              high_orientation_probability=0.2, crystal_size_range=(5, 20), \n",
    "                                                              intensity_variation_range=(0.01, 1), min_mixing_ratio=0.2)\n",
    "    Xs = Xs[:,:-1,:].reshape((Xs.shape[0],int(Xs.shape[1]/visual_embedding_dim),visual_embedding_dim))\n",
    "    \n",
    "    X_prompts = []\n",
    "    for j in range(batch_size):\n",
    "        random.shuffle(element_list[j])\n",
    "        prompt_type = np.random.choice(prompt_type_list)\n",
    "        if prompt_type == None:\n",
    "            X_prompts.append('')\n",
    "        elif prompt_type == 'eds':\n",
    "            X_prompts.append(' '.join([element for element in element_list[j] if element not in ['FA', 'MA']]))\n",
    "        elif prompt_type == 'full':\n",
    "            X_prompts.append(' '.join(element_list[j]))\n",
    "\n",
    "    visual_embeds = torch.from_numpy(Xs).to(dtype=torch.float32)\n",
    "    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "            \n",
    "    inputs = tokenizer(X_prompts, return_tensors=\"pt\", padding=True)\n",
    "    inputs.update(\n",
    "        {\n",
    "            \"visual_embeds\": visual_embeds,\n",
    "            \"visual_token_type_ids\": visual_token_type_ids,\n",
    "            \"visual_attention_mask\": visual_attention_mask,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    labels = torch.from_numpy(np.array(Ys)).to(dtype=torch.float32)\n",
    "\n",
    "    outputs = model(**inputs.to('cuda'), labels=labels.to('cuda'))\n",
    "    logits = outputs.logits\n",
    "    if max_n_mix > 1:\n",
    "        weight = labels*9+1\n",
    "        loss = torch.nn.BCEWithLogitsLoss(weight=weight.to('cuda'))(logits, labels.to('cuda'))\n",
    "    else:\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_true = np.array(labels)\n",
    "    train_y_true.append(y_true)\n",
    "    y_pred = np.array(logits.detach().cpu())\n",
    "    train_y_pred.append(y_pred)\n",
    "    \n",
    "    simp_labels, simp_logits = [], []\n",
    "    results = []\n",
    "    for j in range(batch_size):\n",
    "        simp_labels.append(list(np.array(labels[j].nonzero().view(-1))))\n",
    "        simp_logits.append(list(np.array(torch.topk(outputs.logits[j], len(simp_labels[-1])).indices.detach().cpu())))\n",
    "        if set(simp_labels[-1]) == set(simp_logits[-1]):\n",
    "            results.append(0)\n",
    "        else:\n",
    "            results.append(1)\n",
    "    \n",
    "    print('{}\\t{:.5f}\\t{}'.format(i+1, np.array(loss.detach().cpu()), results))\n",
    "#     break\n",
    "    train_losses.append(np.array(loss.detach().cpu()))\n",
    "    \n",
    "    if (i+1)%5000 == 0:\n",
    "        model.save_pretrained(os.path.join(folder, 'models', model_folder, '{}'.format(i+1)), from_pt=True) \n",
    "        np.savetxt(os.path.join(folder, 'models', model_folder, 'train_loss.csv'), train_losses, delimiter=',')\n",
    "        with open(os.path.join(folder, 'models', model_folder, 'train_result.npy'), 'wb') as handle:\n",
    "            joblib.dump({'y_true':train_y_true, 'y_pred':train_y_pred, 'loss':train_losses}, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Random multi-label train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embedding_dim = 20\n",
    "n_mix = [1,2,3]\n",
    "configuration = VisualBertConfig(vocab_size=len(tokenizer), visual_embedding_dim=visual_embedding_dim, hidden_size=int(Xs.shape[1]/visual_embedding_dim), \n",
    "                                 num_attention_heads=1, num_labels=Ys.shape[1]+len(n_mix))\n",
    "print(configuration)\n",
    "model = VisualBertForQuestionAnswering(configuration).to('cuda')\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_losses,train_y_true,train_y_pred = [],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_epochs = 10\n",
    "\n",
    "batch_size = 20\n",
    "# prompt_type_list = [None,'eds','full']\n",
    "prompt_type_list = ['full']\n",
    "organic_list = ['FA','MA']\n",
    "\n",
    "model_folder = r'20241010 random phase embedding 20 lr1e-4'\n",
    "n_phase = len(dataloader.sample_list)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in range(num_of_epochs):\n",
    "    Xs, Ys, element_list, formula_list, n_mix_list = dataloader.load_data(batch_size=batch_size, n_mix=n_mix, resonable_mixing=True,\n",
    "                                                              peak_probability_range=(1, 1), crystal_size_range=(5, 20), \n",
    "                                                              intensity_variation_range=(0.01, 1), min_mixing_ratio=0.2)\n",
    "    Xs = Xs[:,:-1,:].reshape((Xs.shape[0],int(Xs.shape[1]/visual_embedding_dim),visual_embedding_dim))\n",
    "    n_mix_list_onehot = np.zeros((n_mix_list.size, len(n_mix)))\n",
    "    n_mix_list_onehot[np.arange(n_mix_list.size), n_mix_list-np.min(n_mix)] = 1\n",
    "    Ys = np.concatenate((Ys,n_mix_list_onehot),axis=1)\n",
    "    \n",
    "    X_prompts = []\n",
    "    for j in range(batch_size):\n",
    "        random.shuffle(element_list[j])\n",
    "        prompt_type = np.random.choice(prompt_type_list)\n",
    "        if prompt_type == None:\n",
    "            X_prompts.append('')\n",
    "        elif prompt_type == 'eds':\n",
    "            X_prompts.append(' '.join([element for element in element_list[j] if element not in organic_list]))\n",
    "        elif prompt_type == 'full':\n",
    "            X_prompts.append(' '.join(element_list[j]))\n",
    "\n",
    "    visual_embeds = torch.from_numpy(Xs).to(dtype=torch.float32)\n",
    "    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "            \n",
    "    inputs = tokenizer(X_prompts, return_tensors=\"pt\", padding=True)\n",
    "    inputs.update(\n",
    "        {\n",
    "            \"visual_embeds\": visual_embeds,\n",
    "            \"visual_token_type_ids\": visual_token_type_ids,\n",
    "            \"visual_attention_mask\": visual_attention_mask,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    labels = torch.from_numpy(np.array(Ys)).to(dtype=torch.float32)\n",
    "\n",
    "    outputs = model(**inputs.to('cuda'), labels=labels.to('cuda'))\n",
    "    logits = outputs.logits\n",
    "    if len(n_mix) > 1:\n",
    "        weight = np.array(Ys)\n",
    "        weight[:,-len(n_mix):] = weight[:,-len(n_mix):]*(len(n_mix)-1)\n",
    "        weight[:,-len(n_mix):][weight[:,-len(n_mix):]==0] = 1/(len(n_mix)-1)\n",
    "        phase_weight = n_phase/n_mix_list\n",
    "        phase_weight = np.repeat(np.expand_dims(phase_weight,axis=1),n_phase,1)\n",
    "        for j in range(batch_size):\n",
    "            weight[j,:-len(n_mix)][weight[j,:-len(n_mix)]==1] = (n_phase-n_mix_list[j])/n_mix_list[j]\n",
    "            weight[j,:-len(n_mix)][weight[j,:-len(n_mix)]==0] = (n_phase-n_mix_list[j])/n_phase\n",
    "        loss = torch.nn.BCEWithLogitsLoss(weight=torch.from_numpy(weight).to(dtype=torch.float32).to('cuda'))(logits, labels.to('cuda'))\n",
    "    else:\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_true = np.array(labels)\n",
    "    train_y_true.append(y_true)\n",
    "    y_pred = np.array(logits.detach().cpu())\n",
    "    train_y_pred.append(y_pred)\n",
    "    \n",
    "    simp_labels, simp_logits = [], []\n",
    "    results = []\n",
    "    for j in range(batch_size):\n",
    "        simp_labels.append(list(np.array(labels[j].nonzero().view(-1))))\n",
    "        simp_logits.append(list(np.array(torch.topk(outputs.logits[j], len(simp_labels[-1])).indices.detach().cpu())))\n",
    "        if set(simp_labels[-1]) == set(simp_logits[-1]):\n",
    "            results.append(0)\n",
    "        else:\n",
    "            results.append(1)\n",
    "            \n",
    "    if (i+1)%10 == 0:\n",
    "        print('{}\\t{:.5f}\\t{}'.format(i+1, np.array(loss.detach().cpu()), results))\n",
    "#     break\n",
    "    train_losses.append(np.array(loss.detach().cpu()))\n",
    "    \n",
    "    if (i+1)%50000 == 0:\n",
    "        model.save_pretrained(os.path.join(folder, 'models', model_folder, '{}'.format(i+1)), from_pt=True) \n",
    "        np.savetxt(os.path.join(folder, 'models', model_folder, 'train_loss.csv'), train_losses, delimiter=',')\n",
    "        with open(os.path.join(folder, 'models', model_folder, 'train_result.npy'), 'wb') as handle:\n",
    "            joblib.dump({'y_true':train_y_true, 'y_pred':train_y_pred, 'loss':train_losses}, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nd2]",
   "language": "python",
   "name": "conda-env-nd2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
